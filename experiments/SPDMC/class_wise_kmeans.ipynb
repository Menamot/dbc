{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T12:21:48.165612Z",
     "start_time": "2025-10-17T12:21:47.612224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate data\n",
    "X, y = datasets.make_blobs(\n",
    "    n_samples=[125 * 5, 125 * 2],\n",
    "    n_features=2,\n",
    "    centers=[(9.5, 10), (10, 9.4)],\n",
    "    cluster_std=[[0.6, 0.6], [0.35, 0.3]],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "id": "af447451e53e4142",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Give a first look of compare between no classes_wise Kmeans and classes_wise Kmeans.",
   "id": "1f65af5048ba0235"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T12:21:49.136798Z",
     "start_time": "2025-10-17T12:21:48.169643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dbc import compute_conditional_risk\n",
    "from dbc.main import KmeansDiscreteMinmaxClassifier\n",
    "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# No classes_wise Kmeans\n",
    "DMC_Kmeans = KmeansDiscreteMinmaxClassifier(n_clusters=8)\n",
    "DMC_Kmeans.fit(X_train, y_train)\n",
    "\n",
    "# Train set pred\n",
    "y_pred_train = DMC_Kmeans.predict(X_train)\n",
    "conditional_risk_train = compute_conditional_risk(y_train, y_pred_train)\n",
    "print(f'No classes wise Kmeans DMC train class condition risk: {conditional_risk_train[0]}')\n",
    "\n",
    "# Test set pred\n",
    "y_pred_test = DMC_Kmeans.predict(X_test)\n",
    "conditional_risk_test = compute_conditional_risk(y_test, y_pred_test)\n",
    "print(f'No classes wise Kmeans DMC test class condition risk: {conditional_risk_test[0]}')\n",
    "\n",
    "\n",
    "# Classes_wise Kmeans\n",
    "DMC_Kmeans_classes_wise = KmeansDiscreteMinmaxClassifier(n_clusters={0:6, 1:2}, classes_wise=True)\n",
    "DMC_Kmeans_classes_wise.fit(X_train, y_train)\n",
    "\n",
    "# Train set pred\n",
    "y_pred_train = DMC_Kmeans_classes_wise.predict(X_train)\n",
    "conditional_risk_train = compute_conditional_risk(y_train, y_pred_train)\n",
    "print(f'Classes wise Kmeans DMC train class condition risk: {conditional_risk_train[0]}')\n",
    "\n",
    "# Test set pred\n",
    "y_pred_test = DMC_Kmeans_classes_wise.predict(X_test)\n",
    "conditional_risk_test = compute_conditional_risk(y_test, y_pred_test)\n",
    "print(f'Classes wise Kmeans DMC test class condition risk: {conditional_risk_test[0]}')\n",
    "\n",
    "# Plot the decision regions\n",
    "padding = 1\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "x_min, x_max = X_train[:, 0].min() - padding, X_train[:, 0].max() + padding\n",
    "y_min, y_max = X_train[:, 1].min() - padding, X_train[:, 1].max() + padding\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
    "                     np.arange(y_min, y_max, 0.01))\n",
    "\n",
    "voronoi_plot_2d(Voronoi(DMC_Kmeans.cluster_centers), show_points=False, show_vertices=False, s=1, ax=ax[0])\n",
    "voronoi_plot_2d(Voronoi(DMC_Kmeans_classes_wise.cluster_centers), show_points=False, show_vertices=False, s=1, ax=ax[1])\n",
    "\n",
    "Z_DMC_kmeans = DMC_Kmeans.predict_prob(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z_DMC_kmeans_classes_wise = DMC_Kmeans_classes_wise.predict_prob(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "Z_DMC_kmeans_reshape = np.argmax(Z_DMC_kmeans, axis=1).reshape(xx.shape)\n",
    "Z_DMC_kmeans_classes_wise_reshape = np.argmax(Z_DMC_kmeans_classes_wise, axis=1).reshape(xx.shape)\n",
    "\n",
    "contour1 = ax[0].contourf(xx, yy, Z_DMC_kmeans_reshape, cmap='tab10', alpha=0.6)\n",
    "contour2 = ax[1].contourf(xx, yy, Z_DMC_kmeans_classes_wise_reshape, cmap='tab10', alpha=0.6)\n",
    "\n",
    "colors = ['skyblue', 'firebrick', 'forestgreen']\n",
    "markers = ['o', 's', '^']  # 圆形、方形、三角形\n",
    "\n",
    "for i, (c, m) in enumerate(zip(colors, markers)):\n",
    "    ax[0].scatter(\n",
    "        X_train[y_train == i, 0],\n",
    "        X_train[y_train == i, 1],\n",
    "        color=c,\n",
    "        edgecolor='k',\n",
    "        marker=m,\n",
    "        s=40,\n",
    "        label=f'Class {i}'\n",
    "    )\n",
    "    ax[1].scatter(\n",
    "        X_train[y_train == i, 0],\n",
    "        X_train[y_train == i, 1],\n",
    "        color=c,\n",
    "        edgecolor='k',\n",
    "        marker=m,\n",
    "        s=40,\n",
    "        label=f'Class {i}'\n",
    "    )\n",
    "ax[0].set_title('No classes wise Kmeans DMC')\n",
    "ax[1].set_title('Classes wise Kmeans DMC')\n",
    "plt.colorbar(contour2, ax=ax, ticks=[0, 1, 2], label='Predicted class')\n",
    "plt.show()"
   ],
   "id": "1c127cfef8dc60ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No classes wise Kmeans DMC train class condition risk: [0.07645875 0.45320197]\n",
      "No classes wise Kmeans DMC test class condition risk: [0.0546875  0.44680851]\n",
      "{0: 6, 1: 2}\n",
      "None\n",
      "None\n"
     ]
    },
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'n_clusters' parameter of KMeans must be an int in the range [1, inf). Got None instead.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mInvalidParameterError\u001B[0m                     Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 24\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m# Classes_wise Kmeans\u001B[39;00m\n\u001B[1;32m     23\u001B[0m DMC_Kmeans_classes_wise \u001B[38;5;241m=\u001B[39m KmeansDiscreteMinmaxClassifier(n_clusters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;241m0\u001B[39m:\u001B[38;5;241m6\u001B[39m, \u001B[38;5;241m1\u001B[39m:\u001B[38;5;241m2\u001B[39m}, classes_wise\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m---> 24\u001B[0m DMC_Kmeans_classes_wise\u001B[38;5;241m.\u001B[39mfit(X_train, y_train)\n\u001B[1;32m     26\u001B[0m \u001B[38;5;66;03m# Train set pred\u001B[39;00m\n\u001B[1;32m     27\u001B[0m y_pred_train \u001B[38;5;241m=\u001B[39m DMC_Kmeans_classes_wise\u001B[38;5;241m.\u001B[39mpredict(X_train)\n",
      "File \u001B[0;32m~/PycharmProjects/dbc/dbc/main.py:93\u001B[0m, in \u001B[0;36mBaseDiscreteBayesianClassifier.fit\u001B[0;34m(self, X, y, loss_function)\u001B[0m\n\u001B[1;32m     91\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid loss_function\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprior \u001B[38;5;241m=\u001B[39m compute_prior(y_encoded, n_classes)\n\u001B[0;32m---> 93\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit_discretization(X, y_encoded, n_classes)\n",
      "File \u001B[0;32m~/PycharmProjects/dbc/dbc/main.py:326\u001B[0m, in \u001B[0;36m_KmeansDiscretization._fit_discretization\u001B[0;34m(self, X, y, n_classes)\u001B[0m\n\u001B[1;32m    321\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    322\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdiscretization_model \u001B[38;5;241m=\u001B[39m ClasswiseKMeans(\n\u001B[1;32m    323\u001B[0m         n_clusters_per_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_clusters,\n\u001B[1;32m    324\u001B[0m         random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrandom_state,\n\u001B[1;32m    325\u001B[0m     )\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdiscretization_model\u001B[38;5;241m.\u001B[39mfit(X, y)\n\u001B[1;32m    327\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcluster_centers \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdiscretization_model\u001B[38;5;241m.\u001B[39mcenters_\n\u001B[1;32m    328\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mp_hat \u001B[38;5;241m=\u001B[39m compute_p_hat(\n\u001B[1;32m    329\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdiscretization_model\u001B[38;5;241m.\u001B[39mlabels_,\n\u001B[1;32m    330\u001B[0m         y,\n\u001B[1;32m    331\u001B[0m         n_classes,\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdiscretization_model\u001B[38;5;241m.\u001B[39mn_clusters,\n\u001B[1;32m    333\u001B[0m     )\n",
      "File \u001B[0;32m~/PycharmProjects/dbc/dbc/main.py:210\u001B[0m, in \u001B[0;36mClasswiseKMeans.fit\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m    205\u001B[0m     k \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_clusters_per_class\n\u001B[1;32m    207\u001B[0m kmeans \u001B[38;5;241m=\u001B[39m KMeans(\n\u001B[1;32m    208\u001B[0m     n_clusters\u001B[38;5;241m=\u001B[39mk, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrandom_state, algorithm\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39malgorithm\n\u001B[1;32m    209\u001B[0m )\n\u001B[0;32m--> 210\u001B[0m kmeans\u001B[38;5;241m.\u001B[39mfit(Xc)\n\u001B[1;32m    211\u001B[0m kmeans\u001B[38;5;241m.\u001B[39mfit(Xc)\n\u001B[1;32m    213\u001B[0m centers\u001B[38;5;241m.\u001B[39mappend(kmeans\u001B[38;5;241m.\u001B[39mcluster_centers_)\n",
      "File \u001B[0;32m/opt/miniconda3/envs/RML/lib/python3.11/site-packages/sklearn/base.py:1382\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1377\u001B[0m partial_fit_and_fitted \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   1378\u001B[0m     fit_method\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpartial_fit\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m _is_fitted(estimator)\n\u001B[1;32m   1379\u001B[0m )\n\u001B[1;32m   1381\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m global_skip_validation \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m partial_fit_and_fitted:\n\u001B[0;32m-> 1382\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1384\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1385\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1386\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1387\u001B[0m     )\n\u001B[1;32m   1388\u001B[0m ):\n\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fit_method(estimator, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/miniconda3/envs/RML/lib/python3.11/site-packages/sklearn/base.py:436\u001B[0m, in \u001B[0;36mBaseEstimator._validate_params\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    428\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_validate_params\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    429\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001B[39;00m\n\u001B[1;32m    430\u001B[0m \n\u001B[1;32m    431\u001B[0m \u001B[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    434\u001B[0m \u001B[38;5;124;03m    accepted constraints.\u001B[39;00m\n\u001B[1;32m    435\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 436\u001B[0m     validate_parameter_constraints(\n\u001B[1;32m    437\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_parameter_constraints,\n\u001B[1;32m    438\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_params(deep\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m),\n\u001B[1;32m    439\u001B[0m         caller_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m,\n\u001B[1;32m    440\u001B[0m     )\n",
      "File \u001B[0;32m/opt/miniconda3/envs/RML/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:98\u001B[0m, in \u001B[0;36mvalidate_parameter_constraints\u001B[0;34m(parameter_constraints, params, caller_name)\u001B[0m\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     93\u001B[0m     constraints_str \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     94\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([\u001B[38;5;28mstr\u001B[39m(c)\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39mconstraints[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m or\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     95\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconstraints[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     96\u001B[0m     )\n\u001B[0;32m---> 98\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m InvalidParameterError(\n\u001B[1;32m     99\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparam_name\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m parameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcaller_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    100\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconstraints_str\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparam_val\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    101\u001B[0m )\n",
      "\u001B[0;31mInvalidParameterError\u001B[0m: The 'n_clusters' parameter of KMeans must be an int in the range [1, inf). Got None instead."
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import make_scorer\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from dbc.utils import compute_conditional_risk\n",
    "\n",
    "\n",
    "def make_preprocessor(X):\n",
    "    num_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "    cat_features = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "    num_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "    cat_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", num_transformer, num_features),\n",
    "            (\"cat\", cat_transformer, cat_features)\n",
    "        ]\n",
    "    )\n",
    "    return preprocessor\n",
    "\n",
    "def global_risk(y_true, y_pred):\n",
    "    return np.mean(y_true != y_pred)\n",
    "\n",
    "def max_gap(y_true, y_pred):\n",
    "    R = compute_conditional_risk(y_true, y_pred)[0]\n",
    "    return np.max(R) - np.min(R)\n",
    "\n",
    "def variances_Rk(y_true, y_pred):\n",
    "    R = compute_conditional_risk(y_true, y_pred)[0]\n",
    "    return np.var(R)\n",
    "\n",
    "def max_Rk(y_true, y_pred):\n",
    "    R = compute_conditional_risk(y_true, y_pred)[0]\n",
    "    return np.max(R)\n",
    "\n",
    "def print_results(results):\n",
    "    print(\"Train global_risk: %.3f (± %.3f)\" %\n",
    "          (-results[\"train_global_risk\"].mean(), results[\"train_global_risk\"].std()))\n",
    "    print(\"Train max_Rk: %.3f (± %.3f)\" %\n",
    "          (-results[\"train_max_Rk\"].mean(), results[\"train_max_Rk\"].std()))\n",
    "    print(\"Train max_gap: %.3f (± %.3f)\" %\n",
    "          (-results[\"train_max_gap\"].mean(), results[\"train_max_gap\"].std()))\n",
    "    print(\"Train variances_Rk: %.3f (± %.3f)\" %\n",
    "          (-results[\"train_variances_Rk\"].mean(), results[\"train_variances_Rk\"].std()))\n",
    "\n",
    "    print(\"Test global_risk: %.3f (± %.3f)\" %\n",
    "          (-results[\"test_global_risk\"].mean(), results[\"test_global_risk\"].std()))\n",
    "    print(\"Test max_Rk: %.3f (± %.3f)\" %\n",
    "          (-results[\"test_max_Rk\"].mean(), results[\"test_max_Rk\"].std()))\n",
    "    print(\"Test max_gap: %.3f (± %.3f)\" %\n",
    "          (-results[\"test_max_gap\"].mean(), results[\"test_max_gap\"].std()))\n",
    "    print(\"Test variances_Rk: %.3f (± %.3f)\" %\n",
    "          (-results[\"test_variances_Rk\"].mean(), results[\"test_variances_Rk\"].std()))\n",
    "\n",
    "scoring = {\n",
    "    \"global_risk\": make_scorer(global_risk, greater_is_better=False),\n",
    "    \"max_Rk\": make_scorer(max_Rk, greater_is_better=False),\n",
    "    \"max_gap\": make_scorer(max_gap, greater_is_better=False),\n",
    "    \"variances_Rk\": make_scorer(variances_Rk, greater_is_better=False),\n",
    "}\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=20, random_state=42)\n",
    "\n",
    "\n",
    "# df = fetch_ucirepo(id=17)  # Breast Cancer Wisconsin\n",
    "# df = fetch_ucirepo(id=15)  # Breast Cancer Wisconsin\n",
    "# df = fetch_ucirepo(id=53)  # IRIS\n",
    "# df = fetch_ucirepo(id=186)  # Wine quality\n",
    "# df = fetch_ucirepo(id=2)  # Adult\n",
    "# df = fetch_ucirepo(id=222)  # Bank Marketing\n",
    "# df = fetch_ucirepo(id=19)  # Car Evaluation(表现很差)\n",
    "\n",
    "# SPDMC参数似乎是越大越好70 1.4，没测试更高的\n",
    "# df = fetch_ucirepo(id=59)  # Letter Recognition(不知道为什么这个数据集DMC和SPDMC效果很差,难道是分类类别过多导致的？)\n",
    "df = fetch_ucirepo(id=149)  # Statlog (Vehicle Silhouettes)(如果SPDMC不收敛记得移除类别只有1个的)\n",
    "\n",
    "X = df.data.features.replace(\"?\", np.nan)  # Only for adult\n",
    "\n",
    "X = df.data.features\n",
    "X = make_preprocessor(X).fit_transform(X)\n",
    "y = df.data.targets.values.ravel()\n",
    "\n",
    "# Onlu for Statlog\n",
    "mask = y != '204'\n",
    "X = X.iloc[mask] if hasattr(X, 'iloc') else X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "np.bincount(y)\n"
   ],
   "id": "44ec0e5b560b5ace",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def optimal_kmeans_by_silhouette(X, k_range=(2, 10), random_state=0, plot=False):\n",
    "    \"\"\"\n",
    "    使用轮廓系数（Silhouette Score）自动选择最优聚类数的 KMeans 聚类。\n",
    "\n",
    "    参数:\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            数据矩阵。\n",
    "        k_range : tuple(int, int)\n",
    "            要测试的聚类数范围 (min_k, max_k)。\n",
    "        random_state : int\n",
    "            随机种子，保证结果可复现。\n",
    "        plot : bool\n",
    "            是否绘制轮廓系数随 K 变化的曲线。\n",
    "\n",
    "    返回:\n",
    "        best_k : int\n",
    "            最优聚类数。\n",
    "        best_model : sklearn.cluster.KMeans\n",
    "            对应的最优 KMeans 模型。\n",
    "        silhouette_scores : dict\n",
    "            每个 K 对应的轮廓系数得分。\n",
    "    \"\"\"\n",
    "    min_k, max_k = k_range\n",
    "    silhouette_scores = {}\n",
    "\n",
    "    for k in range(min_k, max_k + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=random_state)\n",
    "        labels = kmeans.fit_predict(X)\n",
    "        score = silhouette_score(X, labels)\n",
    "        silhouette_scores[k] = score\n",
    "        if plot:\n",
    "            print(f\"K={k}: Silhouette score = {score:.4f}\")\n",
    "\n",
    "    # 选择最高得分的 k\n",
    "    best_k = max(silhouette_scores, key=silhouette_scores.get)\n",
    "    best_model = KMeans(n_clusters=best_k, random_state=random_state).fit(X)\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.plot(list(silhouette_scores.keys()), list(silhouette_scores.values()), 'o-', color='navy')\n",
    "        plt.axvline(best_k, color='red', linestyle='--', label=f'Best K = {best_k}')\n",
    "        plt.xlabel(\"Number of clusters (K)\")\n",
    "        plt.ylabel(\"Silhouette Score\")\n",
    "        plt.title(\"Optimal K Selection by Silhouette Coefficient\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return best_k, best_model, silhouette_scores\n",
    "\n",
    "def optimal_kmeans_by_silhouette_all_class(X, y, k_range=(2, 10), random_state=0, plot=False):\n",
    "    n_classes = len(np.unique(y))\n",
    "    clusters_dict = {}\n",
    "    for i in range(n_classes):\n",
    "        clusters_dict[i] = optimal_kmeans_by_silhouette(X[y==i], k_range=(2, 12))[0]\n",
    "    return clusters_dict\n",
    "\n",
    "dict = optimal_kmeans_by_silhouette_all_class(X,y)"
   ],
   "id": "7778fe3c4fd5edeb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "results_DMC_classes_wise = cross_validate(\n",
    "    KmeansDiscreteMinmaxClassifier(n_clusters=dict, classes_wise=True),\n",
    "    X, y,\n",
    "    cv=rskf,\n",
    "    scoring=scoring,\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1,\n",
    ")"
   ],
   "id": "e458491a2787e026",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.utils import estimator_html_repr\n",
    "from dbc.main import KmeansDiscreteMinmaxClassifier\n",
    "\n",
    "est = KmeansDiscreteMinmaxClassifier(n_clusters=dict)\n",
    "print(est.get_params())"
   ],
   "id": "5861e7f773f373a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3c828ec011abc2d5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
